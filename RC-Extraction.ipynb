{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RC-Extraction.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1gL3_HdB-QDyhV15eAEeAwqTLtON0yP24","authorship_tag":"ABX9TyNJIsd61sdk1PDrjv2vb1We"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0Ta8t086nqrn","colab_type":"text"},"source":["INSTALLATION"]},{"cell_type":"code","metadata":{"id":"bAOhN_bOZLd1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"d85b6cca-d480-4463-f2c4-5cb2ce9596bd","executionInfo":{"status":"ok","timestamp":1588665648709,"user_tz":-330,"elapsed":2375,"user":{"displayName":"Sanya Varghese","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLGgiQEHWOf3v3nAJJ-OCYXHyewi79blu0bmRPv0=s64","userId":"08744062089258615812"}}},"source":["import pycocotools\n","import os\n","#os.chdir(\"/content/drive/My Drive/0000/Object_Detection\")\n","#!git clone https://github.com/pytorch/vision.git\n","os.chdir(\"/content/drive/My Drive/0000/Object_Detection/vision\")\n","'''\n","#!git checkout v0.3.0 \n","!cp references/detection/utils.py ../\n","!cp references/detection/transforms.py ../\n","!cp references/detection/coco_eval.py ../\n","!cp references/detection/engine.py ../\n","!cp references/detection/coco_utils.py ../\n","'''\n"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#!git checkout v0.3.0 \\n!cp references/detection/utils.py ../\\n!cp references/detection/transforms.py ../\\n!cp references/detection/coco_eval.py ../\\n!cp references/detection/engine.py ../\\n!cp references/detection/coco_utils.py ../\\n'"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"EaAhP01AnxIH","colab_type":"text"},"source":["IMPORTS"]},{"cell_type":"code","metadata":{"id":"jXhqM7Z2ZSOC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":278},"outputId":"181983b0-8384-4635-ab42-3bff12ee4be2","executionInfo":{"status":"ok","timestamp":1588665653497,"user_tz":-330,"elapsed":7125,"user":{"displayName":"Sanya Varghese","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLGgiQEHWOf3v3nAJJ-OCYXHyewi79blu0bmRPv0=s64","userId":"08744062089258615812"}}},"source":["import numpy as np\n","import torch\n","import torch.utils.data\n","from PIL import Image\n","import pandas as pd\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","# IMPORTS FOR THE LABEL IN THE FORM OF COCO DATASET\n","import argparse\n","import collections\n","import datetime\n","import glob\n","import json\n","import os.path as osp\n","import sys\n","\n","import PIL.Image\n","\n","! pip install labelme\n"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: labelme in /usr/local/lib/python3.6/dist-packages (4.2.10)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from labelme) (1.9.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from labelme) (3.2.1)\n","Requirement already satisfied: imgviz>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from labelme) (1.0.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from labelme) (1.1.0)\n","Requirement already satisfied: Pillow>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from labelme) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from labelme) (1.18.3)\n","Requirement already satisfied: PyQt5 in /usr/local/lib/python3.6/dist-packages (from labelme) (5.14.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from labelme) (3.13)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->labelme) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->labelme) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->labelme) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->labelme) (1.2.0)\n","Requirement already satisfied: PyQt5-sip<13,>=12.7 in /usr/local/lib/python3.6/dist-packages (from PyQt5->labelme) (12.7.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->labelme) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YxlQ7dD2APnH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"outputId":"237fc3fc-e8f2-4b84-9875-4dd304c577c4","executionInfo":{"status":"error","timestamp":1588667428190,"user_tz":-330,"elapsed":1860,"user":{"displayName":"Sanya Varghese","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLGgiQEHWOf3v3nAJJ-OCYXHyewi79blu0bmRPv0=s64","userId":"08744062089258615812"}}},"source":["class labelme2coco(object):\n","    def __init__(self, labelme_json=[], save_json_path=\"./coco.json\"):\n","        \"\"\"\n","        :param labelme_json: the list of all labelme json file paths\n","        :param save_json_path: the path to save new json\n","        \"\"\"\n","        self.labelme_json = labelme_json\n","        self.save_json_path = save_json_path\n","        self.images = []\n","        self.categories = []\n","        self.annotations = []\n","        self.label = []\n","        self.annID = 1\n","        self.height = 0\n","        self.width = 0\n","\n","        self.save_json()\n","\n","    def data_transfer(self):\n","        for num, json_file in enumerate(self.labelme_json):\n","            with open(json_file, \"r\") as fp:\n","                data = json.load(fp)\n","                self.images.append(self.image(data, num))\n","                for shapes in data[\"shapes\"]:\n","                    label = shapes[\"label\"].split(\"_\")\n","                    if label not in self.label:\n","                        self.label.append(label)\n","                    points = shapes[\"points\"]\n","                    self.annotations.append(self.annotation(points, label, num))\n","                    self.annID += 1\n","\n","        # Sort all text labels so they are in the same order across data splits.\n","        self.label.sort()\n","        for label in self.label:\n","            self.categories.append(self.category(label))\n","        for annotation in self.annotations:\n","            annotation[\"category_id\"] = self.getcatid(annotation[\"category_id\"])\n","\n","    def image(self, data, num):\n","        image = {}\n","        img = utils.img_b64_to_arr(data[\"imageData\"])\n","        height, width = img.shape[:2]\n","        img = None\n","        image[\"height\"] = height\n","        image[\"width\"] = width\n","        image[\"id\"] = num\n","        image[\"file_name\"] = data[\"imagePath\"].split(\"/\")[-1]\n","\n","        self.height = height\n","        self.width = width\n","\n","        return image\n","\n","    def category(self, label):\n","        category = {}\n","        category[\"supercategory\"] = label[0]\n","        category[\"id\"] = len(self.categories)\n","        category[\"name\"] = label[0]\n","        return category\n","\n","    def annotation(self, points, label, num):\n","        annotation = {}\n","        contour = np.array(points)\n","        x = contour[:, 0]\n","        y = contour[:, 1]\n","        area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n","        annotation[\"segmentation\"] = [list(np.asarray(points).flatten())]\n","        annotation[\"iscrowd\"] = 0\n","        annotation[\"area\"] = area\n","        annotation[\"image_id\"] = num\n","\n","        annotation[\"bbox\"] = list(map(float, self.getbbox(points)))\n","\n","        annotation[\"category_id\"] = label[0]  # self.getcatid(label)\n","        annotation[\"id\"] = self.annID\n","        return annotation\n","\n","    def getcatid(self, label):\n","        for category in self.categories:\n","            if label == category[\"name\"]:\n","                return category[\"id\"]\n","        print(\"label: {} not in categories: {}.\".format(label, self.categories))\n","        exit()\n","        return -1\n","\n","    def getbbox(self, points):\n","        polygons = points\n","        mask = self.polygons_to_mask([self.height, self.width], polygons)\n","        return self.mask2box(mask)\n","\n","    def mask2box(self, mask):\n","\n","        index = np.argwhere(mask == 1)\n","        rows = index[:, 0]\n","        clos = index[:, 1]\n","\n","        left_top_r = np.min(rows)  # y\n","        left_top_c = np.min(clos)  # x\n","\n","        right_bottom_r = np.max(rows)\n","        right_bottom_c = np.max(clos)\n","\n","        return [\n","            left_top_c,\n","            left_top_r,\n","            right_bottom_c - left_top_c,\n","            right_bottom_r - left_top_r,\n","        ]\n","\n","    def polygons_to_mask(self, img_shape, polygons):\n","        mask = np.zeros(img_shape, dtype=np.uint8)\n","        mask = PIL.Image.fromarray(mask)\n","        xy = list(map(tuple, polygons))\n","        PIL.ImageDraw.Draw(mask).polygon(xy=xy, outline=1, fill=1)\n","        mask = np.array(mask, dtype=bool)\n","        return mask\n","\n","    def data2coco(self):\n","        data_coco = {}\n","        data_coco[\"images\"] = self.images\n","        data_coco[\"categories\"] = self.categories\n","        data_coco[\"annotations\"] = self.annotations\n","        return data_coco\n","\n","    def save_json(self):\n","        print(\"save coco json\")\n","        self.data_transfer()\n","        self.data_coco = self.data2coco()\n","\n","        print(self.save_json_path)\n","        os.makedirs(\n","            os.path.dirname(os.path.abspath(self.save_json_path)), exist_ok=True\n","        )\n","        json.dump(self.data_coco, open(self.save_json_path, \"w\"), indent=4)\n","\n","\n","if __name__ == \"__main__\":\n","    import argparse\n","\n","    parser = argparse.ArgumentParser(\n","        description=\"labelme annotation to coco data json file.\"\n","    )\n","    parser.add_argument(\n","        \"labelme_images\",\n","        help=\"Directory to labelme images and annotation json files.\",\n","        type=str,\n","    )\n","    parser.add_argument(\n","        \"--output\", help=\"Output json file path.\", default=\"trainval.json\"\n","    )\n","    args = parser.parse_args()\n","    labelme_json = glob.glob(os.path.join(args.labelme_images, \"*.json\"))\n","    labelme2coco(labelme_json, args.output)"],"execution_count":60,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--output OUTPUT] images-annotations\n","ipykernel_launcher.py: error: unrecognized arguments: -f\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kZFzZiFU-BLg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":133},"outputId":"6478d151-3af0-4670-83cd-4b532c6ad003","executionInfo":{"status":"error","timestamp":1588667428195,"user_tz":-330,"elapsed":1757,"user":{"displayName":"Sanya Varghese","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaLGgiQEHWOf3v3nAJJ-OCYXHyewi79blu0bmRPv0=s64","userId":"08744062089258615812"}}},"source":["class PennFundanDataset(object):\n","        def __init__(self, root, transforms):\n","            self.root = root\n","            print(root)\n","            self.transforms = transforms\n","            # load all image files, sorting them to\n","            # ensure that they are aligned\n","            self.imgs = list(sorted(os.listdir(os.path.join(root, \"JPEGImages\"))))\n","            self.masks = list(sorted(os.listdir(os.path.join(root, \"SegmentationObjectPNG\"))))\n","        def __getitem__(self, idx):\n","        # load images ad masks\n","        img_path = os.path.join(self.root, \"JPEGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"SegmentationObjectPNG\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # note that we haven't converted the mask to RGB,\n","        # because each color corresponds to a different instance\n","        # with 0 being background\n","        mask = Image.open(mask_path)\n","\n","        mask = np.array(mask)\n","        # instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # first id is the background, so remove it\n","        obj_ids = obj_ids[1:]\n","        \n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        masks = mask == obj_ids[:, None, None]\n","        # np.savetxt('test.out', masks, delimiter=',')\n","        # get bounding box coordinates for each mask\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # there is only one class\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","    \n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model\n","\n","        def get_transform(train):\n","            transforms = []\n","            transforms.append(T.ToTensor())\n","            if train:\n","                transforms.append(T.RandomHorizontalFlip(0.3))\n","            return T.Compose(transforms)\n","        \n","        def main():\n","            num_classes = 2    \n","            dataset = PennFudanDataset('credit', get_transform(train=True))\n","    dataset_test = PennFudanDataset('credit', get_transform(train=False))\n","    # split the dataset in train and test set\n","    indices = torch.randperm(len(dataset)).tolist()\n","    dataset = torch.utils.data.Subset(dataset, indices)\n","    dataset_test = torch.utils.data.Subset(dataset_test, indices)\n","\n","    # define training and validation data loaders\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=2, shuffle=True, num_workers=4,\n","        collate_fn=utils.collate_fn)\n","\n","    data_loader_test = torch.utils.data.DataLoader(\n","        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n","        collate_fn=utils.collate_fn)\n","\n","    # get the model using our helper function\n","    model = get_model_instance_segmentation(num_classes)\n","\n","    # move model to the right device\n","    model.to(device)\n","\n","    # construct an optimizer\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.0001,\n","                                momentum=0.9, weight_decay=0.00001)\n","    # and a learning rate scheduler\n","    \"\"\"\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                   step_size=3,\n","                                                   gamma=0.1)\n","    \"\"\"\n","    # let's train it for 10 epochs\n","    num_epochs = 200\n","\n","    model_file = \"model_saved.pkl\"\n","\n","    for epoch in range(num_epochs):\n","        # train for one epoch, printing every 10 iterations\n","        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=4)\n","        # update the learning rate\n","        #lr_scheduler.step()\n","        # evaluate on the test dataset\n","        evaluate(model, data_loader_test, device=device)\n","        with open(os.path.join(\"output\", model_file), \"wb\") as f:\n","            torch.save(model.state_dict(), f)\n","            print(\"Model saved :\", model_file)\n","\n","    print(\"That's it!\")\n","    \n","if __name__ == \"__main__\":\n","    main()"],"execution_count":61,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-01775e6c4767>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    img_path = os.path.join(self.root, \"JPEGImages\", self.imgs[idx])\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"]}]},{"cell_type":"code","metadata":{"id":"gvXpANAJctnD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}